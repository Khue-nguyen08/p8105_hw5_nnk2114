---
title: "p8105_hw5_nnk2114"
output: github_document
---

```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(rvest)
library(readxl)
set.seed(1)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 8,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1
```{r}
bday_sim = function(n_room) {
  birthdays = sample (1:365, n_room, replace = TRUE)
  repeated_bday = length(unique(birthdays)) <n_room
repeated_bday
}

```

View the results

```{r}
bday_sim_results =
  expand_grid(
    bdays = 5: 50,
    iter = 1:10000
  )|>
  mutate(
    results = map_lgl(bdays, bday_sim)
  ) |>
  group_by(
    bdays
  ) |>
  summarize(
    prob_repeat= mean(results)
  )

```

Plot this
```{r}
bday_sim_results |>
  ggplot(aes( x= bdays, y = prob_repeat)) +
   geom_line(color = "blue") +
  geom_point()+
     geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") +
  labs(title = "Probability of Shared Birthday vs Group Size",
       x = "Group Size",
       y = "Probability of Shared Birthday") +
  theme_minimal()
```


## Problem 2

Define the function
```{r}
sim_t_test = function(samp_size = 30, mu = 0, sigma = 5, alpha = 0.05){
  
  results = 
    map(1:100, ~ rnorm(n = samp_size, mean = mu, sd = sigma)) |> 
    map(~ t.test(.x, mu = 0)) |> 
    map_df(~ broom::tidy(.x)) |> 
    select(estimate, p.value) |> 
    mutate(reject_null = p.value < alpha)
  
  return(results)
}

```

Run for mu = (1,2,3,4,5,6)

```{r}
t_test_results =
  expand_grid(mu = 0:6) |> 
  mutate(result = map(mu, ~ sim_t_test(mu = .x))) |> 
  unnest(cols = c(result))
```

Make the power plot

```{r}
power_plot = 
t_test_results |> 
  group_by(mu) |> 
  summarize(
    proportion = mean(reject_null)
  )
```

```{r}
power_plot |> 
  ggplot(aes(x = mu, y = proportion)) +
  geom_point(color = "red") +
  geom_line(color = "darkred") +
   labs(
    title = "Power Curve of One-Sample t-test",
    x = expression("True mean ("*mu*")"),
    y = "Power"
  ) +
  theme_minimal(base_size = 14)
```


When μ increases, the power of the test also increases which shows that the probability of rejecting the null hypothesis increases for larger effect sizes. The test is sensitive to the differences between the 0 and the true μ, thus the test tend to rejects null hypothesis more when the true μ increases far from 0. When μ is closer to 0, the test rarely rejects the null hypothesis. 
 
Compute the average

```{r}
avg_estimates =
  t_test_results |>
  group_by(mu) |>
  summarize(
    mean_mu_all = mean(estimate),
    mean_mu_reject = mean(estimate[reject_null]),
    .groups = "drop"
  )
```

```{r}
avg_estimates |>
  ggplot(aes(x = mu)) +
  geom_line(aes(y = mean_mu_all, color = "All samples")) +
  geom_point(aes(y = mean_mu_all, color = "All samples")) +
  geom_line(aes(y = mean_mu_reject, color = "Null rejected"), linetype = "dashed") +
  geom_point(aes(y = mean_mu_reject, color = "Null rejected")) +
  scale_color_manual(values = c("All samples" = "darkblue", "Null rejected" = "red")) +
   scale_x_continuous(breaks = 0:6) +
  labs(
    title = "Average Estimate of μ hat by True Mean (μ)",
    x = expression("True mean ("*mu*")"),
    y = expression("Average estimate ("*hat(mu)*")"),
    color = " "
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
```

The estimated average for all samples are close to the true mean showing that the estimator is not biased. However, the estimated mean average for the samples with the null being rejected is higher than the true mean at the small effect sizes. Since we tested based on the significance, so the larger effect size often had the null rejected. 


## Problem 3



```{r}
hw5_data= 
  read_csv("homicide-data.csv") |>
  janitor::clean_names()|>
  mutate(
  city_state = 
  str_c(city, state, sep = ", ")
  )
hw5_homicides =
  hw5_data|> 
  group_by(city)|>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))
    
hw5_homicides |>
  arrange(desc(total_homicides)) |> 
  knitr::kable()


```

The raw dataset contains information about murders in U.S. cities, include  `r nrow(hw5_homicides)` observations and `r ncol(hw5_homicides)` variables.

- `uid`: case ID

- `victim_last`, `victim_first`,`victim_age`, `victim_sex`, `victim_race`: victims' information

- `city`, `state`: location of the muders

- `disposition`: status if the case is solved or not


Estimate the proportion of homicides that are unsolved for the city of Baltimore, MD


```{r}
hw5_homicides |>
  filter(city == "Baltimore") |>
  summarize (
    prop = list(prop.test(unsolved_homicides, total_homicides))
  )|>
  
   mutate(
     results = map(prop, broom::tidy)
     )|>
  unnest(results) |>
  select(estimate, conf.low, conf.high)|>
  knitr::kable(digits = 3)
```

Run the prop test for each of the cities



```{r}
all_city_prop=
  hw5_homicides |> 
   mutate(
     prop = map2(unsolved_homicides,total_homicides, ~ prop.test(x = .x, n = .y, correct = FALSE))
    ) |> 
  mutate(tidy_test = map(prop, broom::tidy)) |> 
  select(city, tidy_test) |> 
  unnest(cols = tidy_test) |> 
  select(city, estimate, conf.low, conf.high) |> 
  arrange(estimate) 
 
all_city_prop |>  knitr::kable(digits = 4)

```
Plot the proportions
```{r}
all_city_prop |> 
  mutate(city = factor(city, levels = city)) |> 
  ggplot(aes(x = city, y = estimate)) +
  geom_point(color = "blue", size = 1) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.3) +
  labs(
    x = "City",
    y = "Proportion of Unsolved Homicides",
    title = "Proportion of Unsolved Homicides"
  ) +  
  theme_minimal() +
  coord_flip()+
  theme(
    axis.text.y = element_text(size = 4)
  )
```

The graph showed the states with low proportion of unsolved cases are Richmond and Charlotte with less than 30% cases are unsolved. The city with the highest proportion of unsolved cases is Chicago 73%. 
Note that when I grouped the city_state there was 583 cases in Tulsa, OK and one Tulsa, AL. Since there was no Tulsa city in AL, I chose to group by the city instead. I did not correct the data for the state. 
```{r}
hw5_data |>
  select(city, state, city_state)|>
  filter(city == "Tulsa")|>
  group_by(city_state)|>
  count()

```




